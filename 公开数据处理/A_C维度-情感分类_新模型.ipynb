{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T08:44:22.339337400Z",
     "start_time": "2023-11-15T08:44:22.305517400Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-24T14:03:15.318893Z",
     "iopub.status.busy": "2023-11-24T14:03:15.318305Z",
     "iopub.status.idle": "2023-11-24T14:03:15.322843Z",
     "shell.execute_reply": "2023-11-24T14:03:15.321927Z",
     "shell.execute_reply.started": "2023-11-24T14:03:15.318860Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CUDA 11.2\n",
    "# !unzip SEMEVAL-2014-REST.zip\n",
    "# !python -m pip install paddlepaddle-gpu==2.3.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !python -m pip install paddlenlp==2.4.2 -f https://www.paddlepaddle.org.cn/whl/windows/mkl/avx/stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T04:53:09.729365Z",
     "iopub.status.busy": "2023-11-29T04:53:09.728624Z",
     "iopub.status.idle": "2023-11-29T04:53:15.418161Z",
     "shell.execute_reply": "2023-11-29T04:53:15.417169Z",
     "shell.execute_reply.started": "2023-11-29T04:53:09.729334Z"
    },
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-27T14:21:35.642426700Z",
     "start_time": "2023-12-27T14:21:28.690434700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2023-12-27 22:21:31,070] [    INFO]\u001B[0m - Already cached C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\skep_ernie_1.0_large_ch.vocab.txt\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,108] [    INFO]\u001B[0m - tokenizer config file saved in C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\tokenizer_config.json\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,112] [    INFO]\u001B[0m - Special tokens file saved in C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\special_tokens_map.json\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,115] [    INFO]\u001B[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/skep_ernie_1.0_large_ch\\vocab.txt and saved to C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,117] [    INFO]\u001B[0m - Found C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\vocab.txt\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,120] [    INFO]\u001B[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/skep_ernie_1.0_large_ch\\added_tokens.json and saved to C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,121] [    INFO]\u001B[0m - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community/skep_ernie_1.0_large_ch\\added_tokens.json\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,287] [    INFO]\u001B[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/skep_ernie_1.0_large_ch\\special_tokens_map.json and saved to C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,288] [    INFO]\u001B[0m - Found C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\special_tokens_map.json\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,292] [    INFO]\u001B[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/skep_ernie_1.0_large_ch\\tokenizer_config.json and saved to C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,293] [    INFO]\u001B[0m - Found C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\tokenizer_config.json\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:31,645] [    INFO]\u001B[0m - Already cached C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\skep_ernie_1.0_large_ch.pdparams\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:34,965] [    INFO]\u001B[0m - Already cached C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\skep_ernie_1.0_large_ch.vocab.txt\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:34,973] [    INFO]\u001B[0m - tokenizer config file saved in C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\tokenizer_config.json\u001B[0m\n",
      "\u001B[32m[2023-12-27 22:21:34,975] [    INFO]\u001B[0m - Special tokens file saved in C:\\Users\\93461\\.paddlenlp\\models\\skep_ernie_1.0_large_ch\\special_tokens_map.json\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'舒适': 0, '耐久': 1, '精细': 2, '保障': 3, '牌证': 4, '外观': 5, '外部': 6, '响应及时性': 7, '沟通有效性': 8, '信息安全性': 9, '补偿合理性': 10, '服务专业性': 11}\n",
      "感知质量数据集/ 3\n",
      "cp_感知质量数据集/skep_ernie_1.0_large_ch/_Normal_Lstm_Res_max_8_cross_entropy\n",
      "\t q_length \t 20\n",
      "\t max_seq_len \t 100\n",
      "\t root \t 感知质量数据集/\n",
      "\t model_name \t skep_ernie_1.0_large_ch\n",
      "\t num_epoch \t 15\n",
      "\t batch_size \t 4\n",
      "\t convert \t Normal\n",
      "\t learning_rate \t 2e-05\n",
      "\t num_attention_heads \t 8\n",
      "\t batch_func \t max\n",
      "\t loss_func \t cross_entropy\n",
      "\t lstm \t Lstm\n",
      "\t res \t Res\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_train.csv\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_dev.csv\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_test_.csv\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_pred.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from functions import *\n",
    "from bert_attention import *\n",
    "from bert_bare import *\n",
    "from bert_lstm import *\n",
    "import configparser\n",
    "import os\n",
    "from visualdl import LogWriter\n",
    "from paddlenlp.transformers import SkepTokenizer, SkepModel, RobertaTokenizer, RobertaModel, BertTokenizer, BertModel\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "paddle.set_device(\"gpu:0\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.config\", encoding=\"utf-8\")\n",
    "\n",
    "Root = config.get('Settings','Root')\n",
    "Num_attention_heads = int(config.get('Settings','Num_attention_heads'))\n",
    "Num_epoch = int(config.get('Settings','Num_epoch'))\n",
    "Batch_size = int(config.get('Settings','Batch_size'))\n",
    "if '2014' in Root:\n",
    "    Num_sentis = 4\n",
    "else:\n",
    "    Num_sentis = 3\n",
    "Convert = config.get('Settings','Convert')\n",
    "Max_seq_len = int(config.get('Settings','Max_seq_len'))\n",
    "Q_length = int(config.get('Settings','Q_length'))\n",
    "Learning_rate = float(config.get('Settings','Learning_rate'))\n",
    "Model_name = config.get('Settings','Model_name')\n",
    "Lstm = config.get('Settings','Lstm')\n",
    "Res = config.get('Settings','Res')\n",
    "Batch_func = config.get('Settings','Batch_func')\n",
    "Loss_func = config.get('Settings','Loss_func')\n",
    "\n",
    "label2id = {val:i for i, val in enumerate(open(Root+'label.txt', 'r', encoding='utf-8').read().splitlines())}\n",
    "id2label = {i:val for i, val in enumerate(open(Root+'label.txt', 'r', encoding='utf-8').read().splitlines())}\n",
    "\n",
    "if 'uncased' in Model_name or 'bert-base-chinese' in Model_name:\n",
    "    model = BertModel.from_pretrained(Model_name);Tokenizer = BertTokenizer.from_pretrained(Model_name)\n",
    "elif 'skep' in Model_name:\n",
    "    model = SkepModel.from_pretrained(Model_name);Tokenizer = SkepTokenizer.from_pretrained(Model_name)\n",
    "\n",
    "Hidden_size = model.config['hidden_size']\n",
    "\n",
    "Convert_func =  convert_example_to_feature_normal if Convert == 'Normal' else convert_example_to_feature_special\n",
    "Checkpoint = '_'.join([str(each) for each in ['cp',\n",
    "                                              Root+Model_name+'/',\n",
    "                                              Convert,\n",
    "                                                Lstm,\n",
    "                                                Res,\n",
    "                                                Batch_func,\n",
    "                                                Num_attention_heads,\n",
    "                                                Loss_func\n",
    "                                              ]])\n",
    "Writer = LogWriter(logdir=Checkpoint)\n",
    "os.mkdir(Checkpoint) if not os.path.exists(Checkpoint) else None\n",
    "read_func = file_read_meituan if Root == '美团中文数据集/' else file_read_semeval\n",
    " \n",
    "print(label2id)\n",
    "print(Root,Num_sentis)\n",
    "print(Checkpoint)\n",
    "for each in config.items('Settings'):\n",
    "    print('\\t',each[0],'\\t',each[1])\n",
    "Train_loader,Dev_loader,Test_loader,Pred_loader = create_dataloader(Tokenizer,read_func,Convert_func,Root,Batch_size,['train','dev','test_','pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-27T12:37:04.046378Z",
     "iopub.status.busy": "2023-11-27T12:37:04.045826Z",
     "iopub.status.idle": "2023-11-27T12:37:22.802668Z",
     "shell.execute_reply": "2023-11-27T12:37:22.801851Z",
     "shell.execute_reply.started": "2023-11-27T12:37:04.046342Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-27T14:24:56.755959100Z",
     "start_time": "2023-12-27T14:24:36.522171100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取文件路径： 感知质量数据集/关键词抽取结果_train.csv\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_dev.csv\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_test_.csv\n",
      "读取文件路径： 感知质量数据集/关键词抽取结果_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:17<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp_感知质量数据集/skep_ernie_1.0_large_ch/_Normal_Lstm_Res_max_8_cross_entropy\n",
      "维度-情感分类效果\n",
      " P: 0.759 R: 0.712 F1: 0.734 S_G: 416 len_S: 548 len_G: 584\n",
      "维度-分类效果\n",
      " P: 0.885 R: 0.830 F1: 0.856 S_G: 485 len_S: 548 len_G: 584\n",
      "情感-分类效果\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9141    0.8098    0.8588       184\n",
      "           2     0.7215    0.8382    0.7755       136\n",
      "           3     0.9329    0.9273    0.9301       165\n",
      "\n",
      "    accuracy                         0.8577       485\n",
      "   macro avg     0.8562    0.8584    0.8548       485\n",
      "weighted avg     0.8665    0.8577    0.8597       485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练与测试\n",
    "from tqdm import tqdm\n",
    "Train_loader,Dev_loader,Test_loader,Pred_loader = create_dataloader(Tokenizer,read_func,Convert_func,Root,Batch_size,['train','dev','test_','pred'])\n",
    "Lr_scheduler = paddle.optimizer.lr.LinearWarmup(learning_rate=Learning_rate,warmup_steps=int(Num_epoch * 0.1 * len(Train_loader)),start_lr=0,end_lr=Learning_rate,verbose=False)\n",
    "Model = Bert_LSTM_Attention(model,Hidden_size,len(label2id),Num_attention_heads,Num_sentis,Batch_func,Lstm,Res)\n",
    "Optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=Lr_scheduler,\n",
    "    parameters=Model.parameters(),\n",
    "    weight_decay=0.01,\n",
    "    apply_decay_param_fun=lambda x: x in [p.name for n, p in Model.named_parameters()if not any(nd in n for nd in [\"bias\", \"norm\"])])\n",
    "# train_bert_attention(Model, Train_loader, Num_epoch,  Writer, Lr_scheduler, Optimizer, Checkpoint, 20, Max_seq_len, Num_sentis, Loss_func)\n",
    "\n",
    "def test_bert_attention(Model, test_loader,  max_seqlength, checkpoint, num_sentis, root):\n",
    "    Model.set_state_dict(paddle.load(f\"{checkpoint}/best_cls_bert_attention.pdparams\"))\n",
    "    Model.eval()\n",
    "    id2label = {i:val for i, val in enumerate(open(root+'label.txt', 'r', encoding='utf-8').read().splitlines())}\n",
    "    with paddle.no_grad():\n",
    "        Labels_3=[];Preds_3=[]  # 用于情感分类\n",
    "        Labels_4=[];Preds_4=[]  # 用于维度分类\n",
    "        Labels_5=[];Preds_5=[]  # 用于维度-情感分类\n",
    "        S_G=0;len_S=0;len_G=0   # 用于维度-情感分类\n",
    "        S_G2=0;len_S2=0;len_G2=0   # 用于维度-情感分类\n",
    "\n",
    "        for batch_data in tqdm(test_loader()):\n",
    "            input_ids_data, input_ids_term, attention_mask_input_data, attention_mask_input_term, senti_labels = batch_data\n",
    "            state, outputs_bert, cross_attention = Model(input_ids_data,\n",
    "                                                                  input_ids_term,\n",
    "                                                                  attention_mask_input_data,\n",
    "                                                                  attention_mask_input_term,\n",
    "                                                                  max_seqlength)\n",
    "            # 把模型输出转为独热的形式\n",
    "            state2_ = paddle.reshape(state, [len(state), -1, num_sentis + 1])\n",
    "            state2_ = paddle.nn.functional.softmax(state2_, axis=-1)\n",
    "            # 对于每一个样本\n",
    "            for each_sample in range(0,len(state2_)):\n",
    "\n",
    "                state2_pred_3 = [];labels_3=[]  # 用于情感分类\n",
    "                state2_pred_4 = [];labels_4=[]  # 用于维度分类\n",
    "                state2_pred_5 = [];labels_5=[]  # 用于维度-情感分类\n",
    "\n",
    "                # 对于每一个标签\n",
    "                for i in range(0,len(state2_[each_sample])):\n",
    "                    # 处理维度-情感分类，和维度分类\n",
    "                    state = state2_[each_sample][i]\n",
    "\n",
    "                    # 对于预测值的每一个维度，如果不是0，就把维度和情感值拼接起来\n",
    "                    if int(paddle.argmax(state)) > 0:\n",
    "                        state2_pred_5.append(str(int(paddle.argmax(state)))+'_'+id2label[i])\n",
    "                        state2_pred_4.append(id2label[i])\n",
    "                        len_S+=1;len_S2+=1\n",
    "\n",
    "                    # 对于标签的每一个维度，如果不是0，就把维度和情感值拼接起来\n",
    "                    if senti_labels[each_sample][i] > 0:\n",
    "                        labels_5.append(str(int(senti_labels[each_sample][i]))+'_'+id2label[i])\n",
    "                        labels_4.append(id2label[i])\n",
    "                        len_G+=1;len_G2+=1\n",
    "\n",
    "                    # 处理情感分类，如果预测和标签的维度都不是0，就把情感加入到预测值列表中\n",
    "                    state2_pred_3.append(paddle.argmax(state)) if paddle.argmax(state) != 0 and senti_labels[each_sample][i] != 0 else None\n",
    "                    labels_3.append(senti_labels[each_sample][i]) if paddle.argmax(state) != 0 and senti_labels[each_sample][i] != 0 else None\n",
    "\n",
    "                Preds_3.extend(state2_pred_3);Labels_3.extend(labels_3)\n",
    "                Preds_4.append(state2_pred_4);Labels_4.append(labels_4)\n",
    "                Preds_5.append(state2_pred_5);Labels_5.append(labels_5)\n",
    "\n",
    "        for each_sample in range(0,len(Preds_5)):\n",
    "            interlist = list(set(Preds_5[each_sample]).intersection(set(Labels_5[each_sample])))\n",
    "            for i in range(0,len(interlist)):\n",
    "                S_G+=1\n",
    "\n",
    "        for each_sample in range(0,len(Preds_4)):\n",
    "            interlist = list(set(Preds_4[each_sample]).intersection(set(Labels_4[each_sample])))\n",
    "            for i in range(0,len(interlist)):\n",
    "                S_G2+=1\n",
    "\n",
    "        P=S_G/len_S\n",
    "        R=S_G/len_G\n",
    "        F1=2*P*R/(P+R) if P+R != 0 else 0\n",
    "\n",
    "        P2=S_G2/len_S2\n",
    "        R2=S_G2/len_G2\n",
    "        F12=2*P2*R2/(P2+R2) if P2+R2 != 0 else 0\n",
    "        print(checkpoint)\n",
    "        print('维度-情感分类效果\\n','P:',str(P)[0:5],'R:',str(R)[0:5],'F1:',str(F1)[0:5],'S_G:',str(S_G)[0:5],'len_S:',len_S,'len_G:',len_G)\n",
    "        print('维度-分类效果\\n','P:',str(P2)[0:5],'R:',str(R2)[0:5],'F1:',str(F12)[0:5],'S_G:',str(S_G2)[0:5],'len_S:',len_S2,'len_G:',len_G2)\n",
    "        print('情感-分类效果',classification_report(Labels_3, Preds_3, digits=4),sep='\\n')\n",
    "\n",
    "        data = np.zeros([3,3])\n",
    "        for i in range(0,len(Labels_3)):\n",
    "            data[Labels_3[i]-1,Preds_3[i]-1] += 1\n",
    "\n",
    "        pd.DataFrame(data,columns=['1','2','3'],index=['1','2','3']).to_csv(checkpoint+'/confusion_matrix.csv')\n",
    "\n",
    "test_bert_attention(Model, Test_loader, Max_seq_len, Checkpoint, Num_sentis, Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 590/590 [00:50<00:00, 11.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# 调整\n",
    "Model.eval()\n",
    "from tqdm import tqdm\n",
    "Model.set_state_dict(paddle.load(f\"{Checkpoint}/best_cls_bert_attention.pdparams\"))\n",
    "def pred_bert_attention_for_one_case(Model, tokenizer, sentence, max_seq_len, checkpoint, num_sentis, root):\n",
    "\n",
    "    Model.eval()\n",
    "\n",
    "    id2label = {i: val for i, val in enumerate(open(root + 'label.txt', 'r', encoding='utf-8').read().splitlines())}\n",
    "    label2id = {val: i for i, val in enumerate(open(root + 'model_label_cls.dict', 'r', encoding='utf-8').read().splitlines())}\n",
    "    id2label2 = {str(i): val for i, val in enumerate(open(root + 'model_label_cls.dict', 'r', encoding='utf-8').read().splitlines())}\n",
    "\n",
    "    encoded_inputs_data = tokenizer(sentence.split(',')[2],max_length=max_seq_len,truncation=True,padding='max_length')\n",
    "    encoded_inputs_term = tokenizer(sentence.split(',')[1],max_length=max_seq_len,truncation=True,padding='max_length')\n",
    "    encoded_inputs_data['attention_mask']=[1 if i!=tokenizer.pad_token_id else 0 for i in encoded_inputs_data['input_ids']]\n",
    "    encoded_inputs_term['attention_mask']=[1 if i!=tokenizer.pad_token_id else 0 for i in encoded_inputs_term['input_ids']]\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        input_ids_data = paddle.to_tensor([encoded_inputs_data['input_ids']])\n",
    "        input_ids_term = paddle.to_tensor([encoded_inputs_term['input_ids']])\n",
    "        attention_mask_input_data = paddle.to_tensor([encoded_inputs_data['attention_mask']])\n",
    "        attention_mask_input_term = paddle.to_tensor([encoded_inputs_term['attention_mask']])\n",
    "\n",
    "        state, outputs_bert, cross_attention = Model(input_ids_data,\n",
    "                                                     input_ids_term,\n",
    "                                                     attention_mask_input_data,\n",
    "                                                     attention_mask_input_term,\n",
    "                                                     max_seq_len)\n",
    "\n",
    "        state2_ = paddle.reshape(state, [len(state), -1, num_sentis + 1])\n",
    "        state2_ = paddle.nn.functional.softmax(state2_, axis=-1)\n",
    "        state2_ = state2_[0]\n",
    "        # print(id2label2)\n",
    "        with open(r'D:\\python_common\\代码_感知质量大论文\\公开数据处理\\cp_感知质量数据集\\skep_ernie_1.0_large_ch\\_Normal_Lstm_Res_max_8_cross_entropy\\result_sc.tsv','a+',encoding='utf-8') as f:\n",
    "            for i in range(0, len(state2_)):\n",
    "                state = state2_[i]\n",
    "                if int(paddle.argmax(state)) > 0:\n",
    "                    if str(sentence.split(',')[0]) != str(label2id[id2label[i] + str(int(paddle.argmax(state)))]):\n",
    "\n",
    "                        f.write(sentence.split(',')[1] + '\\t' +\n",
    "                                sentence.split(',')[2].strip() + '\\t' +\n",
    "\n",
    "                                sentence.split(',')[0] + '\\t' +\n",
    "                                str(id2label2[sentence.split(',')[0]]) + '\\t' +\n",
    "\n",
    "                                str(label2id[id2label[i] + str(int(paddle.argmax(state)))]) + '\\t' +\n",
    "                                id2label[i] + str(int(paddle.argmax(state))) + '\\n')\n",
    "with open('D:\\python_common\\代码_感知质量大论文\\公开数据处理\\感知质量数据集\\关键词抽取结果_test_.csv','r',encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "for each in tqdm(data):\n",
    "    pred_bert_attention_for_one_case(Model, Tokenizer, each, Max_seq_len, Checkpoint, Num_sentis, Root)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T14:05:13.623296900Z",
     "start_time": "2023-12-27T14:04:20.952426500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "外部3\n",
      "30\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def test_bert_attention_for_one_case(Model, tokenizer, sentence, max_seq_len, checkpoint, num_sentis, root):\n",
    "    Model.set_state_dict(paddle.load(f\"{checkpoint}/best_cls_bert_attention.pdparams\"))\n",
    "    Model.eval()\n",
    "\n",
    "    id2label = {i: val for i, val in enumerate(open(root + 'label.txt', 'r', encoding='utf-8').read().splitlines())}\n",
    "    label2id = {val: i for i, val in enumerate(open(root + 'model_label_cls.dict', 'r', encoding='utf-8').read().splitlines())}\n",
    "\n",
    "    encoded_inputs_data = tokenizer(sentence.split(',')[1],max_length=max_seq_len,truncation=True,padding='max_length')\n",
    "    encoded_inputs_term = tokenizer(sentence.split(',')[0],max_length=max_seq_len,truncation=True,padding='max_length')\n",
    "    encoded_inputs_data['attention_mask']=[1 if i!=tokenizer.pad_token_id else 0 for i in encoded_inputs_data['input_ids']]\n",
    "    encoded_inputs_term['attention_mask']=[1 if i!=tokenizer.pad_token_id else 0 for i in encoded_inputs_term['input_ids']]\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        input_ids_data = paddle.to_tensor([encoded_inputs_data['input_ids']])\n",
    "        input_ids_term = paddle.to_tensor([encoded_inputs_term['input_ids']])\n",
    "        attention_mask_input_data = paddle.to_tensor([encoded_inputs_data['attention_mask']])\n",
    "        attention_mask_input_term = paddle.to_tensor([encoded_inputs_term['attention_mask']])\n",
    "\n",
    "        state, outputs_bert, cross_attention = Model(input_ids_data,\n",
    "                                                     input_ids_term,\n",
    "                                                     attention_mask_input_data,\n",
    "                                                     attention_mask_input_term,\n",
    "                                                     max_seq_len)\n",
    "\n",
    "        state2_ = paddle.reshape(state, [len(state), -1, num_sentis + 1])\n",
    "        state2_ = paddle.nn.functional.softmax(state2_, axis=-1)\n",
    "        state2_ = state2_[0]\n",
    "\n",
    "        for i in range(0, len(state2_)):\n",
    "            state = state2_[i]\n",
    "            if int(paddle.argmax(state)) > 0:\n",
    "                print('------------------')\n",
    "                print(id2label[i]+str(int(paddle.argmax(state))))\n",
    "                print(label2id[id2label[i]+str(int(paddle.argmax(state)))])\n",
    "                print('------------------')\n",
    "\n",
    "test_bert_attention_for_one_case(Model, Tokenizer, '好用非常,保暖性强！非常好用！', Max_seq_len, Checkpoint, Num_sentis, Root)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T13:12:18.446674100Z",
     "start_time": "2023-12-27T13:12:16.169726800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pinpai in ['ALBD','AND','AT','BNL','GRN','LN','QPL','TB']:\n",
    "    Pred_loader = create_dataloader(Tokenizer,read_func,Convert_func,Root,Batch_size,None,pinpai,pred_only=True)\n",
    "    pred_bert_attention(Model, Pred_loader, Max_seq_len, Checkpoint, Num_sentis, Root, pinpai)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT_bare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-24T14:33:03.583400Z",
     "iopub.status.busy": "2023-11-24T14:33:03.583006Z",
     "iopub.status.idle": "2023-11-24T14:33:03.809276Z",
     "shell.execute_reply": "2023-11-24T14:33:03.807860Z",
     "shell.execute_reply.started": "2023-11-24T14:33:03.583373Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_bert_bare() missing 1 required positional argument: 'loss_func'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m Model \u001B[38;5;241m=\u001B[39m Bert_Bare(model,Hidden_size,\u001B[38;5;28mlen\u001B[39m(label2id),Num_sentis)\n\u001B[1;32m      2\u001B[0m Optimizer \u001B[38;5;241m=\u001B[39m paddle\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mAdamW(\n\u001B[1;32m      3\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39mLr_scheduler,\n\u001B[1;32m      4\u001B[0m     parameters\u001B[38;5;241m=\u001B[39mModel\u001B[38;5;241m.\u001B[39mparameters(),\n\u001B[1;32m      5\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,\n\u001B[1;32m      6\u001B[0m     apply_decay_param_fun\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x \u001B[38;5;129;01min\u001B[39;00m [p\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m n, p \u001B[38;5;129;01min\u001B[39;00m Model\u001B[38;5;241m.\u001B[39mnamed_parameters()\u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28many\u001B[39m(nd \u001B[38;5;129;01min\u001B[39;00m n \u001B[38;5;28;01mfor\u001B[39;00m nd \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnorm\u001B[39m\u001B[38;5;124m\"\u001B[39m])])\n\u001B[0;32m----> 7\u001B[0m \u001B[43mtrain_bert_bare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNum_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mWriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLr_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMax_seq_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNum_sentis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m test_bert_bare(Model, Test_loader, Max_seq_len, Checkpoint, Num_sentis, Root)\n",
      "\u001B[0;31mTypeError\u001B[0m: train_bert_bare() missing 1 required positional argument: 'loss_func'"
     ]
    }
   ],
   "source": [
    "Model = Bert_Bare(model,Hidden_size,len(label2id),Num_sentis)\n",
    "Optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=Lr_scheduler,\n",
    "    parameters=Model.parameters(),\n",
    "    weight_decay=0.01,\n",
    "    apply_decay_param_fun=lambda x: x in [p.name for n, p in Model.named_parameters()if not any(nd in n for nd in [\"bias\", \"norm\"])])\n",
    "train_bert_bare(Model, Train_loader, Num_epoch,  Writer, Lr_scheduler, Optimizer, Checkpoint, 20, Max_seq_len, Num_sentis)\n",
    "test_bert_bare(Model, Test_loader, Max_seq_len, Checkpoint, Num_sentis, Root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-24T14:33:03.810054Z",
     "iopub.status.idle": "2023-11-24T14:33:03.810498Z",
     "shell.execute_reply": "2023-11-24T14:33:03.810343Z",
     "shell.execute_reply.started": "2023-11-24T14:33:03.810327Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Model = Bert_LSTM(model, Hidden_size, Num_sentis, len(label2id))\n",
    "Optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=Lr_scheduler,\n",
    "    parameters=Model.parameters(),\n",
    "    weight_decay=0.01,\n",
    "    apply_decay_param_fun=lambda x: x in [p.name for n, p in Model.named_parameters()if not any(nd in n for nd in [\"bias\", \"norm\"])])\n",
    "train_bert_lstm(Model, Train_loader, Num_epoch,  Writer, Lr_scheduler, Optimizer, Checkpoint, 20,  Num_sentis, Loss_func)\n",
    "test_bert_lstm(Model, Test_loader, Max_seq_len, Checkpoint, Num_sentis, Root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "38_paddle",
   "language": "python",
   "display_name": "38_paddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
